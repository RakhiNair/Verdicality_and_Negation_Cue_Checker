{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f6bbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "veridicality_verbs = ['move', 'decline', 'intend', 'attempt', 'start', 'get', 'help',\n",
    "       'manage', 'seek', 'agree', 'expect', 'happen','claim', 'wish',\n",
    "       'wait', 'love', 'work', 'say', 'mean', 'prefer', 'hope', 'prove',\n",
    "       'seem', 'begin', 'use', 'try', 'continue', 'refuse', 'have',\n",
    "       'need', 'want', 'like', 'appear', 'plan', 'tend', 'come', 'choose',\n",
    "       'fail', 'decide', 'remain', 'prepare', 'dare', 'pretend',\n",
    "       'proceed', 'return', 'cease', 'propose', 'deserve', 'exist',\n",
    "       'serve', 'aim', 'strive', 'threaten', 'learn', 'promise', 'ask',\n",
    "       'turn', 'struggle', 'forget', 'reply', 'admit', 'confirm',\n",
    "       'notice', 'imply', 'declare', 'observe', 'predict', 'assert',\n",
    "       'feel', 'insist', 'reveal', 'announce', 'worry', 'see', 'remember',\n",
    "       'demonstrate', 'explain', 'hold', 'complain', 'request',\n",
    "       'speculate', 'add', 'recommend', 'warn', 'understand', 'demand',\n",
    "       'show', 'find', 'note', 'saw', 'know', 'state', 'argue',\n",
    "       'indicate', 'report', 'suggest', 'think', 'believe', 'be',\n",
    "       'recognize', 'realize', 'determine', 'felt', 'conclude', 'assume',\n",
    "       'ensure', 'require', 'estimate', 'comment', 'advise', 'doubt',\n",
    "       'give', 'suspect', 'concern', 'convince', 'acknowledge', 'fear',\n",
    "       'write', 'tell', 'hear', 'contend', 'provide', 'discover',\n",
    "       'mention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9ad63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_list= {'+/+': ['realize that', 'know that', 'remember that','find that',\n",
    "'notice that', 'reveal that', 'acknowledge that', 'admit that', 'learn that', 'observe that', 'see that', 'note that', 'recognize that',\n",
    "'understand that', 'discover that'],\n",
    "'+/−': ['manage to' , 'begin to',\n",
    "'serve to' , 'start to' , 'dare to' ,'use to', 'get to' , 'come to'],\n",
    "'−/+': ['forget to', 'fail to'],\n",
    "'◦/+': ['suspect that', 'explain that' , 'mean to', 'predict that'],\n",
    "'◦/−': ['attempt to'],\n",
    "'−/◦': ['refuse to', 'decline to', 'remain to'],\n",
    "'+/◦': ['show that','confirm that', 'demonstrate that','ensure that',' help to', 'tend to'],\n",
    "'◦/◦': ['try to','hope that','hope to','mention that','like to','continue to','expect that',\n",
    "'agree that','love to','reply that','conclude that','say that',\n",
    "'complain that','speculate that','state that','suggest that',\n",
    "'worry that','mean that','intend to','insist that','imply that',\n",
    "'indicate that','plan to','promise to','prove to','saw that',\n",
    "'seem that','tell that','think that','felt that','write that',\n",
    "'decide to','assume that','believe that','assert that','concern that','estimate that','convince that','decide that','appear that',\n",
    "'argue that','aim to','cease to','strive to','proceed to',\n",
    "'choose to','seem to','prove that','provide that','seek to',\n",
    "'appear to','comment that','contend that','want to','doubt that','feel that','fear that','agree to','announce that','claim that',\n",
    "'struggle to','hear that','propose to','wish to','say to','turn to','wish that','work to','advise that','move to','claim to','expect to','report that','happen to','propose that','hold that','declare that',\n",
    "'prefer to','need to','give that','deserve to','threaten to','exist to','be that','prepare to','wait to','pretend to','ask to','return to','request that','demand that','recommend that','require that']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a8acc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               +/+          +/-          -/+          ◦/+          ◦/−  \\\n",
      "count  1056.000000  1056.000000  1056.000000  1056.000000  1056.000000   \n",
      "mean    110.945076   194.929924    32.607008     8.965909   117.200758   \n",
      "std      74.958521   123.497594    20.288206     8.164701    68.837823   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%      45.000000    96.750000    18.000000     2.000000    58.750000   \n",
      "50%     103.000000   173.000000    27.000000     4.000000   116.000000   \n",
      "75%     176.000000   299.250000    50.000000    18.000000   173.000000   \n",
      "max     254.000000   427.000000    72.000000    24.000000   246.000000   \n",
      "\n",
      "               −/◦          +/◦          ◦/◦  \n",
      "count  1056.000000  1056.000000  1056.000000  \n",
      "mean     35.872159   279.089962  1844.358902  \n",
      "std      24.676557   167.928879  1198.030954  \n",
      "min       0.000000     0.000000     2.000000  \n",
      "25%      13.750000   144.500000   849.500000  \n",
      "50%      32.000000   258.000000  1625.500000  \n",
      "75%      57.000000   423.250000  2838.750000  \n",
      "max      79.000000   600.000000  4146.000000  \n",
      "       veridicality_count\n",
      "count         1056.000000\n",
      "mean            98.268939\n",
      "std            100.715590\n",
      "min              0.000000\n",
      "25%             28.750000\n",
      "50%             69.000000\n",
      "75%            136.000000\n",
      "max           1199.000000\n",
      "{'move': 0.41, 'decline': 0.13, 'intend': 0.06, 'attempt': 0.26, 'start': 0.34, 'get': 1.06, 'help': 1.09, 'manage': 0.16, 'seek': 0.59, 'agree': 0.37, 'expect': 0.16, 'happen': 0.19, 'claim': 0.5, 'wish': 0.2, 'wait': 0.07, 'love': 0.06, 'work': 0.98, 'say': 0.94, 'mean': 0.38, 'prefer': 0.25, 'hope': 0.07, 'prove': 0.32, 'seem': 0.22, 'begin': 0.36, 'use': 2.26, 'try': 0.34, 'continue': 0.94, 'refuse': 0.17, 'have': 24.29, 'need': 0.8, 'want': 0.67, 'like': 0.11, 'appear': 0.43, 'plan': 0.07, 'tend': 0.68, 'come': 1.03, 'choose': 0.65, 'fail': 0.24, 'decide': 0.35, 'remain': 0.74, 'prepare': 0.13, 'dare': 0.0, 'pretend': 0.01, 'proceed': 0.1, 'return': 0.32, 'cease': 0.12, 'propose': 0.11, 'deserve': 0.06, 'exist': 0.96, 'serve': 0.61, 'aim': 0.08, 'strive': 0.03, 'threaten': 0.13, 'learn': 0.4, 'promise': 0.01, 'ask': 0.19, 'turn': 0.22, 'struggle': 0.02, 'forget': 0.03, 'reply': 0.01, 'admit': 0.08, 'confirm': 0.11, 'notice': 0.02, 'imply': 0.13, 'declare': 0.13, 'observe': 0.08, 'predict': 0.1, 'assert': 0.2, 'feel': 0.44, 'insist': 0.05, 'reveal': 0.11, 'announce': 0.02, 'worry': 0.04, 'see': 2.29, 'remember': 0.06, 'demonstrate': 0.18, 'explain': 0.36, 'hold': 0.75, 'complain': 0.04, 'request': 0.04, 'speculate': 0.03, 'add': 0.21, 'recommend': 0.1, 'warn': 0.07, 'understand': 0.31, 'demand': 0.11, 'show': 0.76, 'find': 0.94, 'note': 0.19, 'saw': 0.65, 'know': 0.62, 'state': 0.0, 'argue': 1.24, 'indicate': 0.34, 'report': 0.29, 'suggest': 0.44, 'think': 0.58, 'believe': 1.43, 'be': 32.09, 'recognize': 0.29, 'realize': 0.07, 'determine': 0.57, 'felt': 0.42, 'conclude': 0.11, 'assume': 0.17, 'ensure': 0.76, 'require': 1.09, 'estimate': 0.08, 'comment': 0.02, 'advise': 0.04, 'doubt': 0.03, 'give': 1.04, 'suspect': 0.01, 'concern': 0.01, 'convince': 0.05, 'acknowledge': 0.09, 'fear': 0.11, 'write': 0.19, 'tell': 0.16, 'hear': 0.12, 'contend': 0.14, 'provide': 2.2, 'discover': 0.06, 'mention': 0.07}\n",
      "{'+/+': 254, '+/−': 427, '−/+': 72, '◦/+': 24, '◦/−': 246, '−/◦': 79, '+/◦': 600, '◦/◦': 4146}\n",
      "Most frequent verdicality verb: be\n",
      "Least frequent verdicality verb: dare\n",
      "Most frequent signature_verb: ◦/◦\n",
      "Least frequent signature_verb: ◦/+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# load the IBM Debater – Claim Stance Dataset\n",
    "df = pd.read_csv('article_info.csv')\n",
    "\n",
    "stem_words = []\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "verd_verbs_dict = {key: 0 for key in veridicality_verbs}\n",
    "\n",
    "signature_count = {key: 0 for key in signature_list}\n",
    "\n",
    "for word in veridicality_verbs:\n",
    "    stem_words.append(stemmer.stem(word))\n",
    "\n",
    "\n",
    "def count_veridicality_verbs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    verb_count = 0\n",
    "    for token, pos in pos_tags:\n",
    "        if pos.startswith('V') and stemmer.stem(token.lower()) in stem_words:\n",
    "            verb  = token.lower()\n",
    "            if verb in verd_verbs_dict.keys():\n",
    "                verd_verbs_dict[verb] += 1\n",
    "            verb_count += 1\n",
    "    return verb_count\n",
    "\n",
    "\n",
    "def count_signature_verbs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    for key, values in signature_list.items():\n",
    "        for value in values:\n",
    "            if value in text:\n",
    "                signature_count[key] += 1\n",
    "    return signature_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[['+/+', '+/-', '-/+', '◦/+','◦/−','−/◦','+/◦','◦/◦']] = df['clean_file'].apply(lambda x: pd.Series(count_signature_verbs(x)))\n",
    "\n",
    "print(df[['+/+', '+/-', '-/+', '◦/+','◦/−','−/◦','+/◦','◦/◦']].describe())\n",
    "\n",
    "\n",
    "df['veridicality_count'] = df['clean_file'].apply(lambda x: count_veridicality_verbs(x))\n",
    "\n",
    "print(df[['veridicality_count']].describe())\n",
    "\n",
    "total_verbs_count = sum(verd_verbs_dict.values())\n",
    "verd_verbs_percent = {}\n",
    "\n",
    "for verb, count in verd_verbs_dict.items():\n",
    "    percent = round((count / total_verbs_count) * 100, 2)\n",
    "    verd_verbs_percent[verb] = percent\n",
    "\n",
    "print(verd_verbs_percent)\n",
    "\n",
    "print(signature_count)\n",
    "\n",
    "print(\"Most frequent verdicality verb:\",max(verd_verbs_percent, key=verd_verbs_percent.get))\n",
    "print(\"Least frequent verdicality verb:\",min(verd_verbs_percent, key=verd_verbs_percent.get))\n",
    "\n",
    "print(\"Most frequent signature_verb:\",max(signature_count , key=signature_count.get))\n",
    "print(\"Least frequent signature_verb:\",min(signature_count , key=signature_count.get))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b928bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                +/+           +/-           -/+           ◦/+           ◦/−  \\\n",
      "count  12326.000000  12326.000000  12326.000000  12326.000000  12326.000000   \n",
      "mean     120.726107    125.177917     16.392666      9.346747     37.228541   \n",
      "std       68.064036     77.088981     11.251813      5.231935     21.714693   \n",
      "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%       61.000000     57.000000      6.000000      4.000000     18.000000   \n",
      "50%      118.000000    129.000000     18.000000      9.000000     37.000000   \n",
      "75%      183.000000    194.000000     26.000000     13.000000     58.000000   \n",
      "max      235.000000    251.000000     35.000000     19.000000     71.000000   \n",
      "\n",
      "                −/◦           +/◦           ◦/◦  \n",
      "count  12326.000000  12326.000000  12326.000000  \n",
      "mean      14.329872    142.002515   1319.378874  \n",
      "std        8.774797     84.953227    756.080910  \n",
      "min        0.000000      0.000000      0.000000  \n",
      "25%        5.000000     58.000000    674.000000  \n",
      "50%       15.000000    138.000000   1325.000000  \n",
      "75%       23.000000    217.000000   1983.000000  \n",
      "max       27.000000    282.000000   2600.000000  \n",
      "                +/+           +/-           -/+      ◦/+           ◦/−  \\\n",
      "count  12326.000000  12326.000000  12326.000000  12326.0  12326.000000   \n",
      "mean     235.469982    254.475823     35.811455     19.0     73.718076   \n",
      "std        0.756548      3.279394      0.391162      0.0      1.367110   \n",
      "min      235.000000    251.000000     35.000000     19.0     71.000000   \n",
      "25%      235.000000    251.000000     36.000000     19.0     73.000000   \n",
      "50%      235.000000    255.000000     36.000000     19.0     74.000000   \n",
      "75%      236.000000    255.000000     36.000000     19.0     74.000000   \n",
      "max      237.000000    262.000000     36.000000     19.0     76.000000   \n",
      "\n",
      "                −/◦           +/◦           ◦/◦  \n",
      "count  12326.000000  12326.000000  12326.000000  \n",
      "mean      27.008437    289.805290   2690.259046  \n",
      "std        0.091471      5.626529     55.741287  \n",
      "min       27.000000    282.000000   2600.000000  \n",
      "25%       27.000000    284.000000   2637.000000  \n",
      "50%       27.000000    290.000000   2692.000000  \n",
      "75%       27.000000    292.000000   2734.000000  \n",
      "max       28.000000    301.000000   2789.000000  \n",
      "        argument_id  verdicality_count_premise  verdicality_count_conclusion\n",
      "count  12326.000000               12326.000000                  12326.000000\n",
      "mean    6954.345692                   2.716129                      0.342203\n",
      "std     4061.683805                   2.435800                      0.581010\n",
      "min        0.000000                   0.000000                      0.000000\n",
      "25%     3383.250000                   1.000000                      0.000000\n",
      "50%     6979.500000                   2.000000                      0.000000\n",
      "75%    10410.750000                   4.000000                      1.000000\n",
      "max    14114.000000                  29.000000                      4.000000\n",
      "{'move': 0.48, 'decline': 0.06, 'intend': 0.03, 'attempt': 0.2, 'start': 0.39, 'get': 2.3, 'help': 2.06, 'manage': 0.1, 'seek': 0.53, 'agree': 0.3, 'expect': 0.26, 'happen': 0.34, 'claim': 0.43, 'wish': 0.19, 'wait': 0.13, 'love': 0.24, 'work': 1.2, 'say': 1.44, 'mean': 0.91, 'prefer': 0.27, 'hope': 0.15, 'prove': 0.29, 'seem': 0.44, 'begin': 0.24, 'use': 1.69, 'try': 0.48, 'continue': 0.72, 'refuse': 0.13, 'have': 19.73, 'need': 1.99, 'want': 2.0, 'like': 0.3, 'appear': 0.27, 'plan': 0.05, 'tend': 0.3, 'come': 1.08, 'choose': 0.99, 'fail': 0.36, 'decide': 0.41, 'remain': 0.53, 'prepare': 0.07, 'dare': 0.0, 'pretend': 0.04, 'proceed': 0.04, 'return': 0.29, 'cease': 0.08, 'propose': 0.07, 'deserve': 0.28, 'exist': 0.74, 'serve': 0.47, 'aim': 0.04, 'strive': 0.02, 'threaten': 0.19, 'learn': 0.39, 'promise': 0.03, 'ask': 0.48, 'turn': 0.34, 'struggle': 0.02, 'forget': 0.08, 'reply': 0.01, 'admit': 0.09, 'confirm': 0.04, 'notice': 0.04, 'imply': 0.04, 'declare': 0.09, 'observe': 0.04, 'predict': 0.09, 'assert': 0.08, 'feel': 0.78, 'insist': 0.09, 'reveal': 0.11, 'announce': 0.01, 'worry': 0.1, 'see': 1.67, 'remember': 0.13, 'demonstrate': 0.21, 'explain': 0.13, 'hold': 0.51, 'complain': 0.15, 'request': 0.05, 'speculate': 0.01, 'add': 0.3, 'recommend': 0.02, 'warn': 0.04, 'understand': 0.4, 'demand': 0.17, 'show': 0.63, 'find': 0.9, 'note': 0.08, 'saw': 0.13, 'know': 1.23, 'state': 0.0, 'argue': 0.87, 'indicate': 0.19, 'report': 0.08, 'suggest': 0.28, 'think': 1.33, 'believe': 1.26, 'be': 32.71, 'recognize': 0.33, 'realize': 0.15, 'determine': 0.36, 'felt': 0.1, 'conclude': 0.13, 'assume': 0.24, 'ensure': 0.83, 'require': 0.94, 'estimate': 0.02, 'comment': 0.0, 'advise': 0.03, 'doubt': 0.03, 'give': 1.44, 'suspect': 0.04, 'concern': 0.0, 'convince': 0.05, 'acknowledge': 0.06, 'fear': 0.21, 'write': 0.08, 'tell': 0.43, 'hear': 0.1, 'contend': 0.08, 'provide': 1.4, 'discover': 0.1, 'mention': 0.08}\n",
      "{'+/+': 237, '+/−': 262, '−/+': 36, '◦/+': 19, '◦/−': 76, '−/◦': 28, '+/◦': 301, '◦/◦': 2789}\n",
      "Most frequent verdicality verb: be\n",
      "Least frequent verdicality verb: dare\n",
      "Most frequent signature_verb: ◦/◦\n",
      "Least frequent signature_verb: ◦/+\n"
     ]
    }
   ],
   "source": [
    "# load the Webis-Argument-Framing-19 dataset\n",
    "df = pd.read_csv('Webis-argument-framing.csv')\n",
    "\n",
    "verd_verbs_dict = {key: 0 for key in veridicality_verbs}\n",
    "\n",
    "signature_count = {key: 0 for key in signature_list}\n",
    "\n",
    "\n",
    "def count_veridicality_verbs(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    verb_count = 0\n",
    "    for token, pos in pos_tags:\n",
    "        if pos.startswith('V') and stemmer.stem(token.lower()) in stem_words:\n",
    "            verb  = token.lower()\n",
    "            if verb in verd_verbs_dict.keys():\n",
    "                verd_verbs_dict[verb] += 1\n",
    "            verb_count += 1\n",
    "    return verb_count\n",
    "\n",
    "\n",
    "def count_signature_verbs(text):\n",
    "    for key, values in signature_list.items():\n",
    "        for value in values:\n",
    "            if value in text:\n",
    "                signature_count[key] += 1\n",
    "    return signature_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[['+/+', '+/-', '-/+', '◦/+','◦/−','−/◦','+/◦','◦/◦']] = df['premise'].apply(lambda x: pd.Series(count_signature_verbs(x)))\n",
    "\n",
    "print(df[['+/+', '+/-', '-/+', '◦/+','◦/−','−/◦','+/◦','◦/◦']].describe())\n",
    "\n",
    "\n",
    "df[['+/+', '+/-', '-/+', '◦/+','◦/−','−/◦','+/◦','◦/◦']] = df['conclusion'].apply(lambda x: pd.Series(count_signature_verbs(x)))\n",
    "\n",
    "print(df[['conclusion','+/+', '+/-', '-/+', '◦/+','◦/−','−/◦','+/◦','◦/◦']].describe())\n",
    "\n",
    "\n",
    "df['verdicality_count_premise'] = df['premise'].apply(count_veridicality_verbs)\n",
    "\n",
    "df['verdicality_count_conclusion'] = df['conclusion'].apply(count_veridicality_verbs)\n",
    "\n",
    "\n",
    "print(df[['argument_id', 'verdicality_count_premise','verdicality_count_conclusion']].describe())\n",
    "\n",
    "total_verbs_count = sum(verd_verbs_dict.values())\n",
    "verd_verbs_percent = {}\n",
    "\n",
    "for verb, count in verd_verbs_dict.items():\n",
    "    percent = round((count / total_verbs_count) * 100, 2)\n",
    "    verd_verbs_percent[verb] = percent\n",
    "\n",
    "print(verd_verbs_percent)\n",
    "\n",
    "print(signature_count)\n",
    "\n",
    "print(\"Most frequent verdicality verb:\",max(verd_verbs_percent, key=verd_verbs_percent.get))\n",
    "print(\"Least frequent verdicality verb:\",min(verd_verbs_percent, key=verd_verbs_percent.get))\n",
    "\n",
    "print(\"Most frequent signature_verb:\",max(signature_count , key=signature_count.get))\n",
    "print(\"Least frequent signature_verb:\",min(signature_count , key=signature_count.get))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cae1cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               +/+         +/-          -/+     ◦/+          ◦/−          −/◦  \\\n",
      "count  1029.000000  1029.00000  1029.000000  1029.0  1029.000000  1029.000000   \n",
      "mean     15.186589    11.49174     1.613217     0.0     0.590865     0.862974   \n",
      "std       8.374281     9.59365     1.134042     0.0     0.491913     1.046431   \n",
      "min       0.000000     0.00000     0.000000     0.0     0.000000     0.000000   \n",
      "25%       8.000000     1.00000     1.000000     0.0     0.000000     0.000000   \n",
      "50%      16.000000    11.00000     1.000000     0.0     1.000000     0.000000   \n",
      "75%      22.000000    21.00000     3.000000     0.0     1.000000     2.000000   \n",
      "max      29.000000    28.00000     3.000000     0.0     1.000000     3.000000   \n",
      "\n",
      "               +/◦          ◦/◦  \n",
      "count  1029.000000  1029.000000  \n",
      "mean     25.489796   163.074830  \n",
      "std      11.739187   107.186268  \n",
      "min       0.000000     0.000000  \n",
      "25%      19.000000    64.000000  \n",
      "50%      27.000000   154.000000  \n",
      "75%      34.000000   267.000000  \n",
      "max      47.000000   345.000000  \n",
      "          ARGUMENT  veridicality_count\n",
      "count  1029.000000         1029.000000\n",
      "mean      1.842566            4.419825\n",
      "std       0.794250            2.677663\n",
      "min       1.000000            0.000000\n",
      "25%       1.000000            2.000000\n",
      "50%       2.000000            4.000000\n",
      "75%       2.000000            6.000000\n",
      "max       5.000000           15.000000\n",
      "{'move': 0.24, 'decline': 0.06, 'intend': 0.06, 'attempt': 0.03, 'start': 0.65, 'get': 4.41, 'help': 3.63, 'manage': 0.74, 'seek': 0.06, 'agree': 0.54, 'expect': 0.12, 'happen': 0.27, 'claim': 0.18, 'wish': 0.06, 'wait': 0.09, 'love': 0.27, 'work': 2.44, 'say': 0.6, 'mean': 0.42, 'prefer': 0.6, 'hope': 0.03, 'prove': 0.27, 'seem': 0.21, 'begin': 1.22, 'use': 2.29, 'try': 1.1, 'continue': 0.33, 'refuse': 0.06, 'have': 19.59, 'need': 1.96, 'want': 2.56, 'like': 0.45, 'appear': 0.09, 'plan': 0.0, 'tend': 0.63, 'come': 0.92, 'choose': 1.07, 'fail': 0.27, 'decide': 0.3, 'remain': 0.21, 'prepare': 0.83, 'dare': 0.0, 'pretend': 0.0, 'proceed': 0.03, 'return': 0.06, 'cease': 0.0, 'propose': 0.0, 'deserve': 0.15, 'exist': 0.15, 'serve': 0.36, 'aim': 0.03, 'strive': 0.03, 'threaten': 0.03, 'learn': 4.17, 'promise': 0.0, 'ask': 0.51, 'turn': 0.21, 'struggle': 0.03, 'forget': 0.3, 'reply': 0.0, 'admit': 0.06, 'confirm': 0.0, 'notice': 0.03, 'imply': 0.03, 'declare': 0.0, 'observe': 0.12, 'predict': 0.06, 'assert': 0.06, 'feel': 1.64, 'insist': 0.03, 'reveal': 0.03, 'announce': 0.0, 'worry': 0.21, 'see': 1.43, 'remember': 0.15, 'demonstrate': 0.03, 'explain': 0.24, 'hold': 0.21, 'complain': 0.15, 'request': 0.0, 'speculate': 0.0, 'add': 0.12, 'recommend': 0.0, 'warn': 0.0, 'understand': 1.49, 'demand': 0.0, 'show': 0.92, 'find': 2.08, 'note': 0.0, 'saw': 0.03, 'know': 2.35, 'state': 0.0, 'argue': 0.98, 'indicate': 0.03, 'report': 0.03, 'suggest': 0.09, 'think': 2.2, 'believe': 1.73, 'be': 20.99, 'recognize': 0.09, 'realize': 0.48, 'determine': 0.15, 'felt': 0.3, 'conclude': 0.06, 'assume': 0.0, 'ensure': 0.21, 'require': 0.45, 'estimate': 0.03, 'comment': 0.0, 'advise': 0.0, 'doubt': 0.12, 'give': 1.61, 'suspect': 0.03, 'concern': 0.0, 'convince': 0.09, 'acknowledge': 0.03, 'fear': 0.03, 'write': 0.33, 'tell': 0.21, 'hear': 0.06, 'contend': 0.06, 'provide': 1.99, 'discover': 0.09, 'mention': 0.21}\n",
      "{'+/+': 29, '+/−': 28, '−/+': 3, '◦/+': 0, '◦/−': 1, '−/◦': 3, '+/◦': 47, '◦/◦': 345}\n",
      "Most frequent verdicality verb: be\n",
      "Least frequent verdicality verb: plan\n",
      "Most frequent signature_verb: ◦/◦\n",
      "Least frequent signature_verb: ◦/+\n"
     ]
    }
   ],
   "source": [
    "# load the subset dataset of the collected student essays by Stab and Gurevych\n",
    "df = pd.read_csv('UKP-InsufficientArguments_v1.0/data-tokenized.tsv', sep='\\t', encoding='iso-8859-1')\n",
    "\n",
    "verd_verbs_dict = {key: 0 for key in veridicality_verbs}\n",
    "\n",
    "signature_count = {key: 0 for key in signature_list}\n",
    "\n",
    "def count_veridicality_verbs(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    verb_count = 0\n",
    "    for token, pos in pos_tags:\n",
    "        if pos.startswith('V') and stemmer.stem(token.lower()) in stem_words:\n",
    "            verb  = token.lower()\n",
    "            if verb in verd_verbs_dict.keys():\n",
    "                verd_verbs_dict[verb] += 1\n",
    "            verb_count += 1\n",
    "    return verb_count\n",
    "\n",
    "def count_signature_verbs(text):\n",
    "    for key, values in signature_list.items():\n",
    "        for value in values:\n",
    "            if value in text:\n",
    "                signature_count[key] += 1\n",
    "    return signature_count\n",
    "\n",
    "df[['+/+', '+/-', '-/+', '◦/+','◦/−','−/◦','+/◦','◦/◦']] = df['TEXT'].apply(lambda x: pd.Series(count_signature_verbs(x)))\n",
    "\n",
    "print(df[['TEXT', '+/+', '+/-', '-/+', '◦/+','◦/−','−/◦','+/◦','◦/◦']].describe())\n",
    "\n",
    "\n",
    "df['veridicality_count'] = df['TEXT'].apply(count_veridicality_verbs)\n",
    "\n",
    "\n",
    "print(df[['ARGUMENT', 'veridicality_count']].describe())\n",
    "\n",
    "\n",
    "total_verbs_count = sum(verd_verbs_dict.values())\n",
    "verd_verbs_percent = {}\n",
    "\n",
    "for verb, count in verd_verbs_dict.items():\n",
    "    percent = round((count / total_verbs_count) * 100, 2)\n",
    "    verd_verbs_percent[verb] = percent\n",
    "\n",
    "print(verd_verbs_percent)\n",
    "\n",
    "print(signature_count)\n",
    "\n",
    "print(\"Most frequent verdicality verb:\",max(verd_verbs_percent, key=verd_verbs_percent.get))\n",
    "print(\"Least frequent verdicality verb:\",min(verd_verbs_percent, key=verd_verbs_percent.get))\n",
    "\n",
    "print(\"Most frequent signature_verb:\",max(signature_count , key=signature_count.get))\n",
    "print(\"Least frequent signature_verb:\",min(signature_count , key=signature_count.get))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd37a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
